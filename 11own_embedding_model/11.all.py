print("## 11.1 ì„ë² ë”© ëª¨ë¸ ë§Œë“¤ê¸°")
from sentence_transformers import SentenceTransformer, models
transformer_model = models.Transformer('klue/roberta-base') # 'klue/roberta-base'ëŠ” í•œêµ­ì–´ì— ìµœì í™”ëœ RoBERTa ëª¨ë¸ì…ë‹ˆë‹¤. (ë¡œë²„íƒ€)

pooling_layer = models.Pooling( # í‰ê·  í’€ë§ì¸µì„ ë§Œë“¦
    transformer_model.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True
)

embedding_model = SentenceTransformer(modules=[transformer_model, pooling_layer]) # tranasformer ëª¨ë¸ê³¼ í’€ë§ ë ˆì´ì–´ë¥¼ ê²°í•©í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ì„ ë§Œë“¦
print(embedding_model) # SentenceTransformer ëª¨ë¸ì„ ì¶œë ¥



print("## 11.2 ì‹¤ìŠµ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° í™•ì¸")
from datasets import load_dataset
klue_sts_train = load_dataset('klue', 'sts', split='train') # KLUE STS ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
klue_sts_test = load_dataset('klue', 'sts', split='validation') # KLUE STS ë°ì´í„°ì…‹ì˜ ê²€ì¦ìš© ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
print(klue_sts_train[0]) # ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œì„ ì¶œë ¥í•©ë‹ˆë‹¤.

##
## {'guid': 'klue-sts-v1_train_00000', 'source': 'airbnb-rtt',
#   'sentence1': 'ìˆ™ì†Œ ìœ„ì¹˜ëŠ” ì°¾ê¸° ì‰½ê³  ì¼ë°˜ì ì¸ í•œêµ­ì˜ ë°˜ì§€í•˜ ìˆ™ì†Œì…ë‹ˆë‹¤.',
#   'sentence2': 'ìˆ™ë°•ì‹œì„¤ì˜ ìœ„ì¹˜ëŠ” ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆê³  í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë°˜ì§€í•˜ ìˆ™ë°•ì‹œì„¤ì…ë‹ˆë‹¤.',
#   'labels': {'label': 3.7, 'real-label': 3.714285714285714, 'binary-label': 1}}
##

print( "## 11.3 í•™ìŠµ ë°ì´í„°ì—ì„œ ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬í•˜ê¸°")
klue_sts_train = klue_sts_train.train_test_split(test_size=0.1, seed=42) # í•™ìŠµ ë°ì´í„°ì…‹ì„ 90%ì™€ 10%ë¡œ ë¶„í• í•©ë‹ˆë‹¤.í•™ìŠµë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë™ì¼í•˜ê²Œ ë¶„ë¦¬í•  ìˆ˜ ìˆë„ë¡ seedë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
klue_sts_train, klue_sts_eval = klue_sts_train['train'], klue_sts_train['test'] # ë¶„í• ëœ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.

print( "## 11.4 label ì •ê·œí™”í•˜ê¸°")
from sentence_transformers import InputExample
## ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”í•˜ê³  InputExample ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (í•œë§ˆë””ë¡œ 0 ~ 5 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ 0 ~ 1 ì‚¬ì´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.)
def prepare_sts_examples(dataset):
    examples = []
    for data in dataset:
        examples.append(InputExample(
            texts=[data['sentence1'], data['sentence2']],
            label=data['labels']['label']/5.0)
        )
    return examples

train_examples = prepare_sts_examples(klue_sts_train)
eval_examples = prepare_sts_examples(klue_sts_eval)
test_examples = prepare_sts_examples(klue_sts_test)

print(train_examples[0])


print( "## 11.5 í•™ìŠµì— ì‚¬ìš©í•  ë°°ì¹˜ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°")
from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_examples, batch_size=16, shuffle=True)


print("## 11.6 ê²€ì¦ì„ ìœ„í•œ í‰ê°€ ê°ì²´ ì¤€ë¹„")
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

eval_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(eval_examples)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples)

print("## 11.7 ì–¸ì–´ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ í™œìš©í•  ê²½ìš° ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥")
print(test_evaluator(embedding_model))

print("## 11.8 ì„ë² ë”© ëª¨ë¸ í•™ìŠµ")
from sentence_transformers import losses

num_epochs = 4 # í•™ìŠµ ì—í­ ìˆ˜
model_name = 'klue/roberta-base' # ëª¨ë¸ ì´ë¦„
model_save_path = 'output/training_sts_' + model_name.replace("/", "-")
train_loss = losses.CosineSimilarityLoss(model=embedding_model)

### ì„ë² ë”© ëª¨ë¸ í•™ìŠµ
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

## wandb login í•„ìš”í•©ë‹ˆë‹¤.
# embedding_model.fit(
#     train_objectives=[(train_dataloader, train_loss)],
#     evaluator=eval_evaluator,
#     epochs=num_epochs,
#     evaluation_steps=1000,  # í‰ê°€ ì£¼ê¸° ì„¤ì •
#     warmup_steps=100,
#     output_path=model_save_path,
# )

### ì‹¤í–‰ê²°ê³¼
# wandb: ğŸš€ View run at https://wandb.ai/zbum-nhn-dooray/sentence-transformers/runs/omdhev95
#   0%|          | 0/2628 [00:00<?, ?it/s]/Users/nhn/IdeaProjects/quarkus/rapid_ocr_ex1/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
#   warnings.warn(warn_msg)
#  19%|â–ˆâ–‰        | 500/2628 [05:08<47:12,  1.33s/it]{'loss': 0.0281, 'grad_norm': 0.8412415981292725, 'learning_rate': 1.6838351822503965e-05, 'epoch': 0.76}
#  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1000/2628 [10:25<11:29,  2.36it/s]{'loss': 0.0081, 'grad_norm': 0.3559873104095459, 'learning_rate': 1.287638668779715e-05, 'epoch': 1.52}
# {'eval_pearson_cosine': 0.9597710920882392, 'eval_spearman_cosine': 0.9174741951802741, 'eval_runtime': 5.7564, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 1.52}
#  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1500/2628 [14:39<08:21,  2.25it/s]{'loss': 0.0052, 'grad_norm': 0.2036622315645218, 'learning_rate': 8.914421553090334e-06, 'epoch': 2.28}
#  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2000/2628 [18:56<04:03,  2.58it/s]{'loss': 0.0034, 'grad_norm': 0.27582746744155884, 'learning_rate': 4.952456418383519e-06, 'epoch': 3.04}
#  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2000/2628 [19:01<04:03,  2.58it/s]{'eval_pearson_cosine': 0.962240285336236, 'eval_spearman_cosine': 0.9212081579770475, 'eval_runtime': 5.8192, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 3.04}
#  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2500/2628 [23:12<00:59,  2.15it/s]{'loss': 0.0026, 'grad_norm': 0.22588618099689484, 'learning_rate': 9.904912836767039e-07, 'epoch': 3.81}
# 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2628/2628 [24:42<00:00,  1.77it/s]
# {'train_runtime': 1485.7208, 'train_samples_per_second': 28.272, 'train_steps_per_second': 1.769, 'train_loss': 0.009126491572486755, 'epoch': 4.0}
# ## 11.9 í•™ìŠµí•œ ì„ë² ë”©ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€
# wandb:
# wandb: Run history:
# wandb:     eval/pearson_cosine â–â–ˆ
# wandb:            eval/runtime â–â–ˆ
# wandb: eval/samples_per_second â–â–
# wandb:    eval/spearman_cosine â–â–ˆ
# wandb:   eval/steps_per_second â–â–
# wandb:             train/epoch â–â–ƒâ–ƒâ–„â–†â–†â–ˆâ–ˆ
# wandb:       train/global_step â–â–ƒâ–ƒâ–„â–†â–†â–ˆâ–ˆ
# wandb:         train/grad_norm â–ˆâ–ƒâ–â–‚â–
# wandb:     train/learning_rate â–ˆâ–†â–„â–ƒâ–
# wandb:              train/loss â–ˆâ–ƒâ–‚â–â–
# wandb:
# wandb: Run summary:
# wandb:      eval/pearson_cosine 0.96224
# wandb:             eval/runtime 5.8192
# wandb:  eval/samples_per_second 0.0
# wandb:     eval/spearman_cosine 0.92121
# wandb:    eval/steps_per_second 0.0
# wandb:               total_flos 0.0
# wandb:              train/epoch 4.0
# wandb:        train/global_step 2628
# wandb:          train/grad_norm 0.22589
# wandb:      train/learning_rate 0.0
# wandb:               train/loss 0.0026
# wandb:               train_loss 0.00913
# wandb:            train_runtime 1485.7208
# wandb: train_samples_per_second 28.272
# wandb:   train_steps_per_second 1.769
# wandb:
# wandb: ğŸš€ View run checkpoints/model at: https://wandb.ai/zbum-nhn-dooray/sentence-transformers/runs/omdhev95
# wandb: â­ï¸ View project at: https://wandb.ai/zbum-nhn-dooray/sentence-transformers
# wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
# wandb: Find logs at: ./wandb/run-20250720_145949-omdhev95/logs

print("## 11.9 í•™ìŠµí•œ ì„ë² ë”©ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€")
trained_embedding_model = SentenceTransformer(model_save_path)
print(test_evaluator(trained_embedding_model))

